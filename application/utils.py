import os
import faiss
import torch
from PIL import Image
import numpy
from transformers import AutoProcessor, AutoModel
from qai_hub_models.models.openai_clip.model import Clip
from qai_hub_models.models.mediapipe_face.model import MediaPipeFace
from qai_hub_models.models.mediapipe_face.app import MediaPipeFaceApp
from torchvision import transforms
import pickle

def load_images(folder):
    """
    Loads image file paths from a specified folder.
    Supports image formats: .jpg, .jpeg, .png, .JPG.
    Returns:
        List of image paths.
    """
    images = []
    for filename in os.listdir(folder):
        if filename.endswith(('.jpg', '.jpeg', '.png', '.JPG')):
            image_path = os.path.join(folder, filename)
            images.append(image_path)
    return images

def load_model(device, model_choice):
    """
    Loads a specified model (CLIP or JINA) onto the given device (CPU/GPU).
    
    Args:
        device (str): Target device ('cpu' or 'cuda').
        model_choice (str): Model selection ('CLIP' or 'JINA').
    
    Returns:
        Tuple containing the processor (if applicable) and model.
    """
    if model_choice == "CLIP":
        processor = None
        model = Clip.from_pretrained()
    elif model_choice == "JINA":
        processor = AutoProcessor.from_pretrained("jinaai/jina-clip-v2", trust_remote_code=True)
        model = AutoModel.from_pretrained("jinaai/jina-clip-v2", trust_remote_code=True).to(device)
    else:
        print("Please select a valid model")
        return None
    return processor, model

def transform_image(device, image):
    """
    Transforms an image into a tensor suitable for the CLIP model.
    
    Args:
        device (str): Target device ('cpu' or 'cuda').
        image (PIL.Image): Image to be transformed.
    
    Returns:
        Transformed tensor of the image.
    """
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # Resize to 224x224
        transforms.ToTensor(),          # Convert to tensor
        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])  # Normalize
    ])

    return transform(image).unsqueeze(0).to(device)

def get_image_embedding(image_path, processor, model, device, model_choice):
    """
    Extracts an image embedding using the specified processor and model.
    
    Args:
        image_path (str): Path to the image file.
        processor: Processor for JINA models (None for CLIP).
        model: Loaded CLIP or JINA model.
        device (str): Target device ('cpu' or 'cuda').
        model_choice (str): Model selection ('CLIP' or 'JINA').
    
    Returns:
        Normalized embedding tensor.
    """
    image = Image.open(image_path)
    if model_choice == "CLIP":
        image = transform_image(device, image)
        with torch.no_grad():
            outputs = model.image_encoder.to(device)(image)
    else:
        image = image.convert("RGB").resize((224, 224))
        inputs = processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model.get_image_features(**inputs)
    return torch.nn.functional.normalize(outputs, p=2, dim=1)

def get_text_embedding(text_query, processor, model, device, model_choice):
    """
    Extracts a text embedding using the specified processor and model.
    
    Args:
        text_query (str): Text query for embedding extraction.
        processor: Processor for JINA models (None for CLIP).
        model: Loaded CLIP or JINA model.
        device (str): Target device ('cpu' or 'cuda').
        model_choice (str): Model selection ('CLIP' or 'JINA').
    
    Returns:
        Normalized embedding tensor.
    """
    if model_choice == "CLIP":
        inputs = model.tokenizer_func(text_query).to(device)
        with torch.no_grad():
            outputs = model.text_encoder.to(device)(inputs)
    else:
        inputs = processor(text=text_query, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model.get_text_features(**inputs)
    return torch.nn.functional.normalize(outputs, p=2, dim=1)

def create_faiss_index(embeddings):
    """
    Creates a FAISS index from a set of embeddings for similarity search.
    
    Args:
        embeddings (torch.Tensor): Tensor containing feature embeddings.
    
    Returns:
        FAISS index object.
    """
    embeddings_np = embeddings.numpy().astype('float32')  # Convert to NumPy array
    dimension = embeddings_np.shape[1]
    faiss_index = faiss.IndexFlatIP(dimension)  # Initialize FAISS index for inner product search
    faiss_index.add(embeddings_np)  # Add embeddings to the index
    return faiss_index

def compute_rrf(rank_clip, rank_face, k=60):
    """
    Computes Reciprocal Rank Fusion (RRF) score for two ranked lists.
    
    Args:
        rank_clip (int): Rank of image embedding.
        rank_face (int): Rank of face embedding.
        k (int, optional): Fusion parameter. Default is 60.
    
    Returns:
        RRF score for fusion-based ranking.
    """
    return 1 / (k + rank_clip) + 2 / (k + rank_face)

def normalize_embeddings(embeddings):
    """
    Normalizes embeddings using L2 normalization.
    
    Args:
        embeddings (torch.Tensor): Embeddings to be normalized.
    
    Returns:
        L2-normalized embeddings.
    """
    return torch.nn.functional.normalize(embeddings, p=2, dim=1)

def save_to_pickle(obj, file_path):
    """
    Saves an object to a file using pickle.
    
    Args:
        obj: Python object to save.
        file_path (str): Path to save the pickle file.
    """
    with open(file_path, 'wb') as f:
        pickle.dump(obj, f)

def load_from_pickle(file_path):
    """
    Loads an object from a pickle file.
    
    Args:
        file_path (str): Path to the pickle file.
    
    Returns:
        Loaded object.
    """
    with open(file_path, 'rb') as f:
        obj = pickle.load(f)
    return obj
